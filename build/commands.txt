./zipalign -j0 \
  qwen2-7b-q4.llamafile \
  qwen2-7b-instruct-q4_0.gguf \
  ggml-rocm.so \
  ggml-rocm.dll \
  ggml-cuda.dll \
  ggml-cuda.so \
  .args

./zipalign -j0 \
  qwen2-7b-q3.llamafile \
  qwen2-7b-instruct-q3_k_m.gguf \
  .args

./zipalign -j6 \
  llamafile \
  ggml-rocm.so \
  ggml-rocm.dll \
  ggml-cuda.dll \
  ggml-cuda.so


llamafile -ngl 9999 \
  -m qwen2-7b-instruct-q4_0.gguf \
  -p '[INST
]Write a story about llamas[/INST
]'

rm -rf  $TMPDIR/.llamafile ~/.llamafile

curl http: //192.168.4.128:8083/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer no-key" \
-d '{
  "model": "LLaMA_CPP",
  "stream": true,
  "messages": [
    {
      "role": "system",
      "content": "You are LLAMAfile, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests."
    },
    {
      "role": "user",
      "content": "Write a limerick about python exceptions"
    }
  ]
}'